# Deep-Learning-NLP

BERT : Algorithm https://arxiv.org/abs/1810.04805

# How BERT works
BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms â€” an encoder that reads the text input and a decoder that produces a prediction for the task. 

![image](https://user-images.githubusercontent.com/76734891/185106215-895aca36-c2cc-40ac-b594-ebc3481259d7.png)
![image](https://user-images.githubusercontent.com/76734891/185106257-75d8b249-ebfe-4cd4-90ca-1e97bdbec48a.png)
